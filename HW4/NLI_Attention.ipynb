{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 4: Lab (20 points)\n",
        "\n",
        "## Name: Student 1 (JHED), Student 2 (JHED)"
      ],
      "metadata": {
        "id": "pdlkWv7Ikx16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer to the PDF description for homework 4 for a detailed description of this notebook and the goals of this assignment.\n",
        "\n",
        "You will hand in a PDF of this notebook. Please be sure to clearly answer each question listed in the homework PDF."
      ],
      "metadata": {
        "id": "wxunYldilYpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the code."
      ],
      "metadata": {
        "id": "b7Dz8TkwlzTF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNW1JzOv68Ar",
        "outputId": "88a1c20a-0a6c-4413-8345-ba716021ad59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.10.0\n",
            "  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.2 MB/s eta 0:00:39tcmalloc: large alloc 1147494400 bytes == 0x65cc2000 @  0x7f90877eb615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 51.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (4.1.1)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "Successfully installed sentencepiece-0.1.97 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "# Turn on GPU backend for this Colab by clicking `Runtime > Change Runtime Type` above.\n",
        "\n",
        "!pip install torch==1.10.0\n",
        "!pip install torchtext==0.6.0\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torchtext.data as data\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Neural Network "
      ],
      "metadata": {
        "id": "MgZ80RJjmL41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout):\n",
        "        super(MLP, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(input_dim, output_dim) \n",
        "        self.l2 = torch.nn.Linear(output_dim, output_dim) \n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x).relu()\n",
        "        x = self.dropout(x)\n",
        "        x = self.l2(x).relu()\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class AttentionModel(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 TEXT, LABEL,\n",
        "                 hidden_attn, hidden_aligned, hidden_final,\n",
        "                 dropout = 0.2, device = 'cpu', freeze_emb = True):\n",
        "        super().__init__()\n",
        "        # record parameters\n",
        "        self.device = device\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # initialize embedding\n",
        "        self.pretrained_emb = TEXT.vocab.vectors.to(device)\n",
        "        self.embedding = (\n",
        "            torch.nn.Embedding.from_pretrained(self.pretrained_emb, freeze=freeze_emb)       \n",
        "                ) # seqlen x embedding\n",
        "        self.embedding.weight[1] = torch.zeros(300)\n",
        "        self.embedding_projection = torch.nn.Linear(self.pretrained_emb.shape[1], 200) # embedding x embedding\n",
        "        emb_dim = 200\n",
        "\n",
        "        # initialize feedforward modules\n",
        "        self.feedforward_attn = MLP(emb_dim, hidden_attn, self.dropout) # 'embedding', 'hidden',\n",
        "        self.feedforward_aligned = MLP(2*emb_dim, hidden_aligned, self.dropout)\n",
        "        self.feedforward_agg = MLP(2*hidden_aligned, hidden_final, self.dropout)\n",
        "        self.final_linear = torch.nn.Linear(hidden_final, len(LABEL.vocab))\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, premise, hypothesis):\n",
        "        premise = self.embedding(premise)\n",
        "        premise = self.embedding_projection(premise) #.rename('seqlen', 'seqlenPremise')\n",
        "        hypothesis = self.embedding(hypothesis)\n",
        "        hypothesis = self.embedding_projection(hypothesis) #.rename('seqlen', 'seqlenHypo')\n",
        "\n",
        "        premise_mask = (premise != 0).float()\n",
        "        hypothesis_mask = (hypothesis != 0).float()\n",
        "\n",
        "        #attend\n",
        "        premise_hidden = self.feedforward_attn(premise)\n",
        "        hypothesis_hidden = self.feedforward_attn(hypothesis)\n",
        "        \n",
        "        # matmul: [B, L1, D] * [B, D, L2] -> [B, L1, L2]\n",
        "        # hypothesis_hidden: L1, B, D\n",
        "        # premise_hidden: L2, B, D\n",
        "        hypothesis_hidden = hypothesis_hidden.permute((1,0,2))\n",
        "        premise_hidden = premise_hidden.permute((1,2,0))\n",
        "        self.attn = torch.matmul(hypothesis_hidden, premise_hidden)\n",
        "        #self.attn = premise_hidden.dot(hypothesis_hidden)\n",
        "        # attn.shape: B, L1, L2\n",
        "        self.attn_beta = self.attn.softmax(2)\n",
        "        # premise: L2, B, D -> B, L2, D\n",
        "        # beta: B, L1, D\n",
        "        beta = torch.matmul(self.attn_beta, premise.permute((1,0,2)))\n",
        "        \n",
        "        # hypothesis: L1, B, D -> B, L1, D\n",
        "        # attn: B, L1, L2 -> B, L2, L1\n",
        "        # alpha: B, L2, D\n",
        "        self.attn_alpha = self.attn.softmax(1).permute((0,2,1))\n",
        "        alpha = torch.matmul(self.attn_alpha, hypothesis.permute((1, 0, 2)))\n",
        "\n",
        "        #hypothesis_mask: L1, B, D\n",
        "        # premise_mask: L2, B, D\n",
        "        beta = beta.permute((1, 0, 2)) * hypothesis_mask \n",
        "        alpha = alpha.permute((1, 0, 2)) * premise_mask \n",
        "\n",
        "        #compare\n",
        "        hypothesis_comparison = self.feedforward_aligned(torch.cat([beta, hypothesis], dim=2)).sum(dim=0) # sum along L1\n",
        "        premise_comparison = self.feedforward_aligned(torch.cat([alpha, premise], dim=2)).sum(dim=0) # sum along L2\n",
        "        # B, D\n",
        "\n",
        "        #aggregate\n",
        "        agg = torch.cat([premise_comparison, hypothesis_comparison], dim=1) # cat along , 'hidden'\n",
        "        agg = self.feedforward_agg(agg)\n",
        "        agg = self.final_linear(agg)\n",
        "        return agg, self.attn\n",
        "\n",
        "    def fit(self, train_iter, val_iter=[], lr=0.0001, verbose=True,\n",
        "            batch_size=128, epochs=100, interval=500):\n",
        "        self.to(self.device)\n",
        "        lr = torch.tensor(lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        train_iter.batch_size = batch_size\n",
        "\n",
        "        for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "            running_loss = 0.0\n",
        "            self.train()\n",
        "            for i, data in enumerate(train_iter, 0):\n",
        "                premise, hypothesis, labels = (\n",
        "                    data.premise,\n",
        "                    data.hypothesis,\n",
        "                    data.label\n",
        "                )\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs, _ = self(premise, hypothesis)\n",
        "                loss = criterion(\n",
        "                    outputs,\n",
        "                    labels,\n",
        "                )\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                if i % interval == 0 and i > 0 and verbose:\n",
        "                    print(f\"batch: {i}, loss: {running_loss/interval}\")\n",
        "                    running_loss = 0\n",
        "            if verbose and val_iter is not None:\n",
        "                val_loss = self.validate(val_iter)\n",
        "                print(f\"epoch: {epoch}, val loss: {val_loss}\")\n",
        "        \n",
        "    def validate(self, val_iter):\n",
        "        running_loss = 0\n",
        "        val_count = 0\n",
        "        self.eval()\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for i, data in enumerate(val_iter):\n",
        "            premise, hypothesis, labels = (\n",
        "                data.premise,\n",
        "                data.hypothesis,\n",
        "                data.label\n",
        "            )\n",
        "\n",
        "            outputs, _ = self(premise, hypothesis)\n",
        "            loss = criterion(\n",
        "                outputs,\n",
        "                labels\n",
        "            )\n",
        "            running_loss += loss.item()\n",
        "            val_count += 1\n",
        "        avg_loss = running_loss / val_count\n",
        "        return avg_loss"
      ],
      "metadata": {
        "id": "Nc6_xcDPMXM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "ECMg1hAjmRLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The data\n",
        "# Our input x\n",
        "TEXT = data.Field()\n",
        "\n",
        "# Our labels y\n",
        "LABEL = data.Field(sequential=False,)\n",
        "\n",
        "train, val, test = torchtext.datasets.SNLI.splits(\n",
        "    TEXT, LABEL)\n",
        "\n",
        "pretrained_embedding = 'glove.6B.300d'\n",
        "embedding_dim = 300\n",
        "embedding_num = 100\n",
        "\n",
        "\n",
        "TEXT.build_vocab(train)\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
        "    (train, val, test), batch_size=128, device=torch.device(\"cuda\"), repeat=False, shuffle=False) # cpu\n",
        "\n",
        "# Assign word embeddings.\n",
        "# Out-of-vocabulary (OOV) words are hashed to one of 100 random embeddings. \n",
        "# Initialized to mean 0 and standarad deviation 1.  \n",
        "\n",
        "unk_vectors = [torch.randn(embedding_dim) for _ in range(embedding_num)]\n",
        "TEXT.vocab.load_vectors(vectors=pretrained_embedding, unk_init=lambda x:random.choice(unk_vectors))\n",
        "\n",
        "# Normalize s.t. l_2 norm = 1. \n",
        "\n",
        "vectors = TEXT.vocab.vectors\n",
        "vectors = vectors / vectors.norm(dim=1,keepdim=True)\n",
        "vectors = torch.tensor(vectors) # dims: word, embedding \n",
        "TEXT.vocab.vectors = vectors\n",
        "\n",
        "# Helper function that converts vocabulary ids to string\n",
        "def vocab_id_to_string(ids, FIELD):\n",
        "    string = \"\"\n",
        "    if type(ids) == list:\n",
        "        for id in ids:\n",
        "            if id != 1: # remove <pad>\n",
        "                string += \" \" + FIELD.vocab.itos[id]\n",
        "    elif type(ids) == int:\n",
        "        string = FIELD.vocab.itos[ids]\n",
        "    return string\n",
        "\n",
        "# Helper function that converts string to vocabulary ids\n",
        "def string_to_vocab_ids(string):\n",
        "    vocab_ids = []\n",
        "    for w in string.strip().split():\n",
        "        vocab_ids.append(TEXT.vocab.stoi[w])\n",
        "    return vocab_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH0Qg8dt8IR0",
        "outputId": "f3697ab3-ba4c-4f6b-9c9a-c10f70b3ecc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model\n",
        "\n",
        "Train the model on the training set and save the checkpoint."
      ],
      "metadata": {
        "id": "3R3_9xDlsM72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model_ckpt, epoch=1):\n",
        "    model = AttentionModel(TEXT=TEXT, LABEL=LABEL, hidden_attn=200, hidden_aligned=200, hidden_final=200, device=torch.device(\"cuda\"))\n",
        "    model.fit(train_iter, val_iter, epochs=epoch)\n",
        "    torch.save(model.state_dict(), model_ckpt)"
      ],
      "metadata": {
        "id": "H1cUFyPGsP-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate on Test Set\n",
        "\n",
        "Given a checkpoint, this function would report the accuracy on test set and generate an output file **predictions.txt** which contains information including *sample id*, *premise*, *hypothesis*, *predicted label*, *true label*."
      ],
      "metadata": {
        "id": "hCSgCcfXq8GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model_ckpt):\n",
        "    with torch.no_grad():\n",
        "        model = AttentionModel(TEXT=TEXT, LABEL=LABEL, hidden_attn=200, hidden_aligned=200, hidden_final=200, device=torch.device(\"cuda\"))\n",
        "        model.load_state_dict(torch.load(model_ckpt))\n",
        "        result = []\n",
        "        labels = []\n",
        "\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "        premises = []\n",
        "        hypothesis = []\n",
        "        \n",
        "        test_iter = torchtext.data.BucketIterator(test, train=False, batch_size=10, device=torch.device(\"cuda\"))\n",
        "        for batch in test_iter:\n",
        "            # batch.premise: L2, B\n",
        "            for premise_ids in batch.premise.permute((1,0)):\n",
        "                premises.append(vocab_id_to_string(premise_ids.tolist(), TEXT))\n",
        "            for hypothesis_ids in batch.hypothesis.permute((1,0)):\n",
        "                hypothesis.append(vocab_id_to_string(hypothesis_ids.tolist(), TEXT))\n",
        "                \n",
        "            probs, _ = model(batch.premise, batch.hypothesis)\n",
        "            _, amax = probs.max(dim=1)\n",
        "            result += amax.tolist()\n",
        "            labels += batch.label.tolist()\n",
        "\n",
        "            predictions += [vocab_id_to_string(r, LABEL) for r in result]\n",
        "            true_labels += [vocab_id_to_string(l, LABEL) for l in labels]\n",
        "            \n",
        "        with open(\"predictions.txt\", \"w\") as f:\n",
        "           f.write(\"Id,Premise,Hypothesis,Prediction,True Label\\n\")\n",
        "           for i in range(len(result)):\n",
        "               f.write(str(i) + \",\" + premises[i] + \",\" + hypothesis[i] + \",\" + predictions[i] + \",\" + true_labels[i] + \"\\n\")\n",
        "        \n",
        "        acc = sum(result[i] == labels[i] for i in range(len(result)))\n",
        "        print(\"Accuracy on test set: %.2f\" % ((acc / len(result))*100) + \"%.\")"
      ],
      "metadata": {
        "id": "FCuUOKdA8Ric"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Attention Weights\n",
        "\n",
        "Load a checkpoint, evaluate on a sample and get the attention weights."
      ],
      "metadata": {
        "id": "3K1ddAKLsu63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attentions(model_ckpt, premise, hypothesis):\n",
        "    # Arguments:\n",
        "    #   model_ckpt: the model checkpoint *.pt file\n",
        "    #   premise: a Python string\n",
        "    #   hypothesis: a Python string\n",
        "    # Output:\n",
        "    #   Tensor in the shape of (length_hypothesis, length_premise)\n",
        "    with torch.no_grad():\n",
        "        model = AttentionModel(TEXT=TEXT, LABEL=LABEL, hidden_attn=200, hidden_aligned=200, hidden_final=200, device=torch.device(\"cuda\"))\n",
        "        model.load_state_dict(torch.load(model_ckpt))\n",
        "        premise = string_to_vocab_ids(premise)\n",
        "        hypothesis = string_to_vocab_ids(hypothesis)\n",
        "        premise = torch.tensor(premise).reshape((-1,1)).to(\"cuda\")\n",
        "        hypothesis = torch.tensor(hypothesis).reshape((-1,1)).to(\"cuda\")\n",
        "        _, attn = model(premise, hypothesis)\n",
        "    return attn.squeeze(0).to('cpu') "
      ],
      "metadata": {
        "id": "VMuxaitRr_ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize the Attention Weights"
      ],
      "metadata": {
        "id": "XkK40ejDvoFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_token2head_scores(scores, premise, hypothesis):\n",
        "    premise = premise.split()\n",
        "    hypothesis = hypothesis.split()\n",
        "    fig = plt.figure(figsize=(30, 50))\n",
        "\n",
        "    scores_np = np.array(scores)\n",
        "    ax = fig.add_subplot(6, 2, 1)\n",
        "    # append the attention weights\n",
        "    im = ax.matshow(scores_np, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 20}\n",
        "\n",
        "    ax.set_xticks(range(len(premise)))\n",
        "    ax.set_yticks(range(len(hypothesis)))\n",
        "\n",
        "    ax.set_xticklabels(premise, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels(hypothesis, fontdict=fontdict)\n",
        "\n",
        "    fig.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9DejHEUAhtCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example\n",
        "\n",
        "1. Load a pretrained checkpoint and report the accuracy."
      ],
      "metadata": {
        "id": "AiVvZZ_lv2FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train(\"ckpt3.pt\", epoch=3) You can train your own model using this line of code.\n",
        "test_model(\"ckpt20.pt\") # We have provided you with this checkpoint. You can upload it to the \"content\" directory.\n",
        "# We trained the model for 20 epochs with the given hyperparameters to get this checkpoint."
      ],
      "metadata": {
        "id": "tY42g7el1amx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60b2f81-36f3-4913-d555-bdd0ee173823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 72.78%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming everything went well, you should have an accuray on the test set of around 72.78%. The splits might vary on differnt machines thus the accuracy on test set might also vary."
      ],
      "metadata": {
        "id": "4dGqlTHCn8Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The first step generated a file \"predictions.txt\". We can pick a sample from the file for further analysis. Here, we decided to pick the following sample:\n",
        "\n",
        "```\n",
        "Id, Premise,        Hypothesis,                Prediction,    True Label\n",
        "0,  A biker races., The biker loses the race,  contradiction, neutral\n",
        "```"
      ],
      "metadata": {
        "id": "5HRkHfRpynnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Extract the attention weights."
      ],
      "metadata": {
        "id": "N8QkrZLJ1nPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "premise = \"A biker races.\"\n",
        "hypothesis = \"The biker loses the race\"\n",
        "scores = get_attentions(\"ckpt20.pt\", premise, hypothesis)"
      ],
      "metadata": {
        "id": "EetS_OdM1uOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Visualize the attention weights."
      ],
      "metadata": {
        "id": "yLYuV-RA1_QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_token2head_scores(scores, premise, hypothesis)"
      ],
      "metadata": {
        "id": "713b6NiJ2GUK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "2f7e5b59-7d85-4221-e5a0-844fa02fa2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x3600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answers\n",
        "In the following section, you will answer questions about the trained model on the learned attention weights.\n",
        "\n",
        "You will answer each of the following questions in the notebook. In each question, you will draw a figure that shows the attention weights $e_{ij}$.\n",
        "\n",
        "In each questions, your figure will be a matrix, where an entry for the $i$th row and $j$th column will visualize the weight $e_{ij}$. Darker values mean larger numbers."
      ],
      "metadata": {
        "id": "OQ4w-TrCoKag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.1 (4 points): Draw a figure that shows the attention weights for a correctly classified example from test set for the entailment, contradiction and the neutral classes, respectively. Explain how the learned attention weights contribute to the correct predictions."
      ],
      "metadata": {
        "id": "GNmgyisQfF_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:"
      ],
      "metadata": {
        "id": "-AsUH_A9faA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.2 (4 points): Draw a figure that shows the attention weights for an incorrectly classified example from test set for the entailment, contradiction and the neutral classes, respectively.\n",
        "\n",
        "Describe the difference between the pattern in these attention weights with those of the correctly predicted examples above."
      ],
      "metadata": {
        "id": "0VtmJpfAfcvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:"
      ],
      "metadata": {
        "id": "IgPF4YFkflu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.1 (2 points): Write your own example of an NLI instance (hypothesis and premise)\n",
        "such that the label is ambiguous (i.e. write a confusing/hard example!). Draw a figure of\n",
        "the attention weights and explain how the model’s uncertainty is reflected in the weights."
      ],
      "metadata": {
        "id": "87iz-4QLfnWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:"
      ],
      "metadata": {
        "id": "h_MswIPtft_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.2 (3 points): Make up two examples yourself, such that the two examples have\n",
        "similar words (but need not be identical) but the word order makes a difference in the\n",
        "label. Show the attention weights for each case. Compare the weights in each case and\n",
        "explain how they contribute to the model predictions."
      ],
      "metadata": {
        "id": "oym6L7Edfu-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:"
      ],
      "metadata": {
        "id": "gBM7dZblgEPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.1 (7 points): Randomly initialize the model and start training from scratch. Train\n",
        "for 3 epochs. On a fast GPU, this will be 1 minute per epoch, but it may be slower on\n",
        "older GPUs. Select an example from the training set and plot the attention weights of the\n",
        "model after the third epoch versus those of the fully trained model. Draw a figure of each\n",
        "of the attention weights and describe the difference."
      ],
      "metadata": {
        "id": "oqnBZRVegHpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:"
      ],
      "metadata": {
        "id": "BT_TmlqmgQm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You're Done!\n",
        "\n",
        "You are finished. Print this notebook to a PDF and submit."
      ],
      "metadata": {
        "id": "qh0nMwE2qRIR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U3L6F3roqY5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}