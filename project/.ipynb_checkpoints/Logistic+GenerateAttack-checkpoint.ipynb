{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7MeFJtNalS2"
   },
   "source": [
    "## sklearn and TextAttack\n",
    "\n",
    "This following code trains two different text classification models using sklearn. Both use logistic regression models: the difference is in the features. \n",
    "\n",
    "We will load data using `datasets`, train the models, and attack them using TextAttack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFR3zFvBalS3"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QData/TextAttack/blob/master/docs/2notebook/Example_1_sklearn.ipynb)\n",
    "\n",
    "[![View Source on GitHub](https://img.shields.io/badge/github-view%20source-black.svg)](https://github.com/QData/TextAttack/blob/master/docs/2notebook/Example_1_sklearn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWh7ZuC3alS4"
   },
   "source": [
    "Please remember to run  **pip3 install textattack[tensorflow]**  in your notebook enviroment before the following codes:\n",
    "\n",
    "### Training\n",
    "\n",
    "This code trains two models: one on bag-of-words statistics (`bow_unstemmed`) and one on tfâ€“idf statistics (`tfidf_unstemmed`). The dataset is the IMDB movie review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GJrB3uszalS4",
    "outputId": "b4d2350e-ce28-4dfe-fc91-11a99c671b57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # the Natural Language Toolkit\n",
    "nltk.download('punkt') # The NLTK tokenizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "O1NnLuOca6aX"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "def load_obj(name):\n",
    "    \"\"\"load .pickle\"\"\"\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "Xtrain = load_obj(\"./X_train\")\n",
    "labeltrain = load_obj(\"./y_train\")\n",
    "\n",
    "df_train = pd.DataFrame(Xtrain, columns=['Line'])\n",
    "df_train['Topic'] = labeltrain.astype(int)\n",
    "\n",
    "\n",
    "Xtest = load_obj(\"./X_test\")\n",
    "labeltest = load_obj(\"./y_test\")\n",
    "\n",
    "df_test = pd.DataFrame(Xtest, columns=['Line'])\n",
    "df_test['Topic'] = labeltest.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0E3gGgXfalS5",
    "outputId": "68cc5b54-06b5-4686-f287-02855fd62664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...successfully loaded training data\n",
      "...successfully loaded testing data\n",
      "Total length of training data:  18000\n",
      "...augmented data with len_tokens and average_words\n",
      "Total length of testing data:  6000\n",
      "...augmented data with len_tokens and average_words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/macbookpro/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...successfully created the unstemmed BOW data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbookpro/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of BOW Unstemmed:  0.9831111111111112\n",
      "Testing accuracy of BOW Unstemmed:  0.9696666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       424\n",
      "           1       0.97      0.96      0.96       701\n",
      "           2       0.99      0.96      0.97       462\n",
      "           3       0.96      0.92      0.94       521\n",
      "           4       0.99      0.99      0.99       746\n",
      "           5       0.99      0.99      0.99       396\n",
      "           6       0.95      0.98      0.96      1443\n",
      "           7       0.97      0.95      0.96       396\n",
      "           8       0.98      0.98      0.98       426\n",
      "           9       0.97      0.99      0.98       485\n",
      "\n",
      "    accuracy                           0.97      6000\n",
      "   macro avg       0.97      0.97      0.97      6000\n",
      "weighted avg       0.97      0.97      0.97      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "# Nice to see additional metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def load_data(dataset_split='train'):\n",
    "    dataset = datasets.load_dataset('yahoo_answers_topics')[dataset_split]\n",
    "    l = int(0.01*len(dataset))\n",
    "    dataset = dataset.select(range(l))\n",
    "    # Open and import positve data\n",
    "    df = pd.DataFrame()\n",
    "    df['Line'] = [review['question_title']+review['question_content']+review['best_answer'] for review in dataset]\n",
    "    df['Topic'] = [review['topic'] for review in dataset]\n",
    "    # Remove non-alphanumeric characters\n",
    "    df['Line'] = df['Line'].apply(lambda x: re.sub(\"[^a-zA-Z]\", ' ', str(x)))\n",
    "    # Tokenize the training and testing data\n",
    "    df_tokenized = tokenize_review(df)\n",
    "    return df_tokenized\n",
    "    \n",
    "\n",
    "def tokenize_review(df):\n",
    "    # Tokenize Reviews in training\n",
    "    print(df['Line'][0])\n",
    "    tokened_reviews = [word_tokenize(rev) for rev in df['Line']]\n",
    "    # Create word stems\n",
    "    stemmed_tokens = []\n",
    "    porter = PorterStemmer()\n",
    "    for i in range(len(tokened_reviews)):\n",
    "        stems = [porter.stem(token) for token in tokened_reviews[i]]\n",
    "        stems = ' '.join(stems)\n",
    "        stemmed_tokens.append(stems)\n",
    "    df.insert(1, column='Stemmed', value=stemmed_tokens)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def transform_BOW(training, testing, column_name):\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    ngram_range, min_df, max_df = (1, 2), 0.005, 0.25\n",
    "    vect = CountVectorizer(input='content', decode_error='ignore',\n",
    "                             strip_accents='ascii', lowercase=True,\n",
    "                             stop_words=stopwords, token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_features=1000,\n",
    "                             max_df=max_df, min_df=min_df, ngram_range=ngram_range)\n",
    "    vectFit = vect.fit(training[column_name])\n",
    "    BOW_training = vectFit.transform(training[column_name])\n",
    "    BOW_training_df = pd.DataFrame(BOW_training.toarray(), columns=vect.get_feature_names())\n",
    "    BOW_testing = vectFit.transform(testing[column_name])\n",
    "    BOW_testing_Df = pd.DataFrame(BOW_testing.toarray(), columns=vect.get_feature_names())\n",
    "    return vectFit, BOW_training_df, BOW_testing_Df\n",
    "\n",
    "\n",
    "def add_augmenting_features(df):\n",
    "    tokened_reviews = [word_tokenize(rev) for rev in df['Line']]\n",
    "    # Create feature that measures length of reviews\n",
    "    len_tokens = []\n",
    "    for i in range(len(tokened_reviews)):\n",
    "        len_tokens.append(len(tokened_reviews[i]))\n",
    "    len_tokens = preprocessing.scale(len_tokens)\n",
    "    df.insert(0, column='Lengths', value=len_tokens)\n",
    "\n",
    "    # Create average word length (training)\n",
    "    Average_Words = [len(x)/(len(x.split())) for x in df['Line'].tolist()]\n",
    "    Average_Words = preprocessing.scale(Average_Words)\n",
    "    df['averageWords'] = Average_Words\n",
    "    return df\n",
    "\n",
    "def build_model(X_train, y_train, X_test, y_test, name_of_test):\n",
    "    log_reg = LogisticRegression(C=30, max_iter=200).fit(X_train, y_train)\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    print('Training accuracy of '+name_of_test+': ', log_reg.score(X_train, y_train))\n",
    "    print('Testing accuracy of '+name_of_test+': ', log_reg.score(X_test, y_test))\n",
    "    print(classification_report(y_test, y_pred))  # Evaluating prediction ability\n",
    "    return log_reg\n",
    "\n",
    "\n",
    "#df_train = tokenize_review(train_df)\n",
    "#df_test = tokenize_review(test_df)\n",
    "\n",
    "print('Total length of training data: ', len(df_train))\n",
    "# Add augmenting features\n",
    "df_train = add_augmenting_features(df_train)\n",
    "print('...augmented data with len_tokens and average_words')\n",
    "\n",
    "\n",
    "print('Total length of testing data: ', len(df_test))\n",
    "df_test = add_augmenting_features(df_test) \n",
    "print('...augmented data with len_tokens and average_words')\n",
    "\n",
    "# Create unstemmed BOW features for training set\n",
    "unstemmed_BOW_vect_fit, df_train_bow_unstem, df_test_bow_unstem = transform_BOW(df_train, df_test, 'Line')\n",
    "print('...successfully created the unstemmed BOW data')\n",
    "\n",
    "# Running logistic regression on dataframes\n",
    "bow_unstemmed = build_model(df_train_bow_unstem, df_train['Topic'], df_test_bow_unstem, df_test['Topic'], 'BOW Unstemmed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_T3Q5vralS6"
   },
   "source": [
    "### Attacking\n",
    "\n",
    "TextAttack includes a build-in `SklearnModelWrapper` that can run attacks on most sklearn models. (If your tokenization strategy is different than above, you may need to subclass `SklearnModelWrapper` to make sure the model inputs & outputs come in the correct format.)\n",
    "\n",
    "Once we initializes the model wrapper, we load a few samples from the IMDB dataset and run the `TextFoolerJin2019` attack on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aIrD_agKalS7",
    "outputId": "84482cfc-2820-4836-854f-e35093bf64e7"
   },
   "outputs": [],
   "source": [
    "from textattack.models.wrappers import SklearnModelWrapper\n",
    "\n",
    "model_wrapper = SklearnModelWrapper(bow_unstemmed, unstemmed_BOW_vect_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGeY8Qw842qj"
   },
   "outputs": [],
   "source": [
    "# l = len(df_test)\n",
    "# load  = load.select(range(l))\n",
    "# len(load)\n",
    "# new = []\n",
    "# label = []\n",
    "# for i in load:\n",
    "#   new.append(df_test['Line'])\n",
    "#   label.append(df_test['Topic'])\n",
    "# len(new)\n",
    "# load = load.add_column(name=\"Line\", column = new)\n",
    "# load = load.add_column(name=\"Topic\", column = label)\n",
    "# load = load.remove_columns(['question_title','question_content','best_answer','id','topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /Users/macbookpro/opt/anaconda3/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/macbookpro/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_hub) (1.22.4)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/macbookpro/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_hub) (3.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JBn5q7WCalS7",
    "outputId": "ccdd6ddd-b761-435c-d85e-0cf4496703b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'sklearn.linear_model._logistic.LogisticRegression'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "textattack: Logging to CSV at path log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.840845057\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A/Users/macbookpro/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/macbookpro/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Failed to import tensorflow. Please note that tensorflow is not installed by default when you install tensorflow_hub. This is so that users can decide which tensorflow package to use. To use tensorflow_hub, please install a current version of tensorflow by following the instructions at https://tensorflow.org/install and https://tensorflow.org/hub/installation.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Lazy module loader cannot find module named `tensorflow_hub`. This might be because TextAttack does not automatically install some optional dependencies. Please run `pip install tensorflow_hub` to install the package.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/shared/utils/importing.py:28\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow_hub/__init__.py:85\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis version of tensorflow_hub requires tensorflow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion >= \u001b[39m\u001b[38;5;132;01m{required}\u001b[39;00m\u001b[38;5;124m; Detected an installation of version \u001b[39m\u001b[38;5;132;01m{present}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m             required\u001b[38;5;241m=\u001b[39mrequired_tensorflow_version,\n\u001b[1;32m     83\u001b[0m             present\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39m__version__))\n\u001b[0;32m---> 85\u001b[0m \u001b[43m_ensure_tf_install\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LatestModuleExporter\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow_hub/__init__.py:49\u001b[0m, in \u001b[0;36m_ensure_tf_install\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;66;03m# Print more informative error message, then reraise.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m attack_args \u001b[38;5;241m=\u001b[39m textattack\u001b[38;5;241m.\u001b[39mAttackArgs(\n\u001b[1;32m     15\u001b[0m     num_examples\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     16\u001b[0m     attack_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     log_to_csv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m attacker \u001b[38;5;241m=\u001b[39m Attacker(attack, dataset, attack_args)\n\u001b[0;32m---> 21\u001b[0m att \u001b[38;5;241m=\u001b[39m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/attacker.py:441\u001b[0m, in \u001b[0;36mAttacker.attack_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attack_parallel()\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39msilent:\n\u001b[1;32m    444\u001b[0m     logger\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/attacker.py:170\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack\u001b[38;5;241m.\u001b[39mattack(example, ground_truth_output)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, SkippedAttackResult) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mattack_n\n\u001b[1;32m    173\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, SuccessfulAttackResult)\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_args\u001b[38;5;241m.\u001b[39mnum_successful_examples\n\u001b[1;32m    176\u001b[0m ):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m worklist_candidates:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/attacker.py:168\u001b[0m, in \u001b[0;36mAttacker._attack\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     example\u001b[38;5;241m.\u001b[39mattack_attrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabel_names\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/attack.py:448\u001b[0m, in \u001b[0;36mAttack.attack\u001b[0;34m(self, example, ground_truth_output)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 448\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal_function_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/attack.py:396\u001b[0m, in \u001b[0;36mAttack._attack\u001b[0;34m(self, initial_result)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124;03m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m    ``initial_result``.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;124;03m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     final_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cache()\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_result\u001b[38;5;241m.\u001b[39mgoal_status \u001b[38;5;241m==\u001b[39m GoalFunctionResultStatus\u001b[38;5;241m.\u001b[39mSUCCEEDED:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/search_methods/search_method.py:36\u001b[0m, in \u001b[0;36mSearchMethod.__call__\u001b[0;34m(self, initial_result)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter_transformations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch Method must have access to filter_transformations method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     )\n\u001b[0;32m---> 36\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[1;32m     38\u001b[0m result\u001b[38;5;241m.\u001b[39mnum_queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_function\u001b[38;5;241m.\u001b[39mnum_queries\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/search_methods/greedy_word_swap_wir.py:142\u001b[0m, in \u001b[0;36mGreedyWordSwapWIR.perform_search\u001b[0;34m(self, initial_result)\u001b[0m\n\u001b[1;32m    140\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(index_order) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m search_over:\n\u001b[0;32m--> 142\u001b[0m     transformed_text_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcur_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacked_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mindex_order\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(transformed_text_candidates) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/attack.py:313\u001b[0m, in \u001b[0;36mAttack.get_transformations\u001b[0;34m(self, current_text, original_text, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     transformed_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_transformations_uncached(\n\u001b[1;32m    310\u001b[0m         current_text, original_text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    311\u001b[0m     )\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/attack.py:378\u001b[0m, in \u001b[0;36mAttack.filter_transformations\u001b[0;34m(self, transformed_texts, current_text, original_text)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints_cache[(current_text, transformed_text)]:\n\u001b[1;32m    377\u001b[0m             filtered_texts\u001b[38;5;241m.\u001b[39mappend(transformed_text)\n\u001b[0;32m--> 378\u001b[0m filtered_texts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_transformations_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43muncached_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_text\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Sort transformations to ensure order is preserved between runs\u001b[39;00m\n\u001b[1;32m    382\u001b[0m filtered_texts\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/attack.py:340\u001b[0m, in \u001b[0;36mAttack._filter_transformations_uncached\u001b[0;34m(self, transformed_texts, current_text, original_text)\u001b[0m\n\u001b[1;32m    338\u001b[0m         filtered_texts \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mcall_many(filtered_texts, original_text)\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m         filtered_texts \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Default to false for all original transformations.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m original_transformed_text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/constraints/constraint.py:50\u001b[0m, in \u001b[0;36mConstraint.call_many\u001b[0;34m(self, transformed_texts, reference_text)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformed_text must have `last_transformation` attack_attr to apply constraint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         )\n\u001b[0;32m---> 50\u001b[0m filtered_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_constraint_many\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompatible_transformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_text\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(filtered_texts) \u001b[38;5;241m+\u001b[39m incompatible_transformed_texts\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/constraints/semantics/sentence_encoders/sentence_encoder.py:179\u001b[0m, in \u001b[0;36mSentenceEncoder._check_constraint_many\u001b[0;34m(self, transformed_texts, reference_text)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_constraint_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, transformed_texts, reference_text):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;124;03m\"\"\"Filters the list ``transformed_texts`` so that the similarity\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    between the ``reference_text`` and the transformed text is greater than\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    the ``self.threshold``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, transformed_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transformed_texts):\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m# Optionally ignore similarity score for sentences shorter than the\u001b[39;00m\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# window size.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_text_shorter_than_window\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(transformed_text\u001b[38;5;241m.\u001b[39mwords) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size\n\u001b[1;32m    187\u001b[0m         ):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/constraints/semantics/sentence_encoders/sentence_encoder.py:152\u001b[0m, in \u001b[0;36mSentenceEncoder._score_list\u001b[0;34m(self, starting_text, transformed_texts)\u001b[0m\n\u001b[1;32m    142\u001b[0m     starting_text_windows\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    143\u001b[0m         starting_text\u001b[38;5;241m.\u001b[39mtext_window_around_index(\n\u001b[1;32m    144\u001b[0m             modified_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size\n\u001b[1;32m    145\u001b[0m         )\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m     transformed_text_windows\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    148\u001b[0m         transformed_text\u001b[38;5;241m.\u001b[39mtext_window_around_index(\n\u001b[1;32m    149\u001b[0m             modified_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size\n\u001b[1;32m    150\u001b[0m         )\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 152\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarting_text_windows\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransformed_text_windows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    154\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(embeddings)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/constraints/semantics/sentence_encoders/universal_sentence_encoder/universal_sentence_encoder.py:30\u001b[0m, in \u001b[0;36mUniversalSentenceEncoder.encode\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfhub_url)\n\u001b[1;32m     31\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sentences)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoding, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/shared/utils/importing.py:45\u001b[0m, in \u001b[0;36mLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m---> 45\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, item)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/textattack/shared/utils/importing.py:30\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m     module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazy module loader cannot find module named `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis might be because TextAttack does not automatically install some optional dependencies. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` to install the package.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Update this object's dict so that if someone keeps a reference to the\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#   LazyLoader, lookups are efficient (__getattr__ is only called on lookups\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#   that fail).\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Lazy module loader cannot find module named `tensorflow_hub`. This might be because TextAttack does not automatically install some optional dependencies. Please run `pip install tensorflow_hub` to install the package."
     ]
    }
   ],
   "source": [
    "from textattack.datasets import HuggingFaceDataset\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack import Attacker\n",
    "import textattack\n",
    "import datasets\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "data = Dataset.from_pandas(df_test)\n",
    "dataset = HuggingFaceDataset(data, dataset_columns=(['Line'],'Topic'))\n",
    "\n",
    "attack = TextFoolerJin2019.build(model_wrapper)\n",
    "\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples= 30,\n",
    "    attack_n = True,\n",
    "    log_to_csv=\"log.csv\"\n",
    ")\n",
    "\n",
    "attacker = Attacker(attack, dataset, attack_args)\n",
    "att = attacker.attack_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3v1RGsPyalS7"
   },
   "source": [
    "### Conclusion\n",
    "We were able to train a model on the IMDB dataset using `sklearn` and use it in TextAttack by initializing with the `SklearnModelWrapper`. It's that simple!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
