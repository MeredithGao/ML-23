{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7421a2-7c22-47ab-9a7f-67da05dd7723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.20.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76254c78-a8a9-434e-b602-bd618c547f84",
   "metadata": {},
   "source": [
    "# Download raw data from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9982e9c0-0363-4603-a718-da9004ffcab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"yahoo_answers_topics\")\n",
    "# dataset['train'].to_csv(\"raw_data/train.csv\")\n",
    "# dataset['test'].to_csv(\"raw_data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7e0ad-1b1a-4847-b0dd-779be3195870",
   "metadata": {},
   "source": [
    "# Lemmatize sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee275ced-8216-40e7-9a40-84178af61bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos_tagger(nltk_tag):\n",
    "    \"\"\"Add tag about the grammatical category of each word\"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    \"\"\"Lemmatize sentence. The returned sentence contains letters only. Other characters are removed.\"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+') # include letters only r'[a-zA-Z]+'; r'\\w+'\n",
    "    nltk_tagged = nltk.pos_tag(tokenizer.tokenize(sentence))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_pos_tagger(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    \n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    \"\"\"save as .pickle\"\"\"\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    \"\"\"load .pickle\"\"\"\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74392ff0-5a05-4c9e-9653-221a351b9e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If the light have not be turn off the house would have catch on fire'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example sentence\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatize_sentence(\"If the light had not been turned off, the house would have catched on fire.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f25bb8-4723-4f7a-ad21-ab670e39c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lemmatize(file_path, output_prefix, row_per_time):\n",
    "    \"\"\"Lemmatize and save sentences in batches.\"\"\"\n",
    "    skiprows = 0 # start from 0 (inclusive)\n",
    "    # total number of rows\n",
    "    num_lines = int(subprocess.check_output(\"wc -l \"+file_path, shell=True).split()[0]) - 1\n",
    "    print(\"Processing {} ({} lines): \".format(file_path, num_lines))\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    while (skiprows < num_lines):\n",
    "        print(\"{} - {}:\".format(skiprows, skiprows+row_per_time), end=\" \")\n",
    "        # read in data\n",
    "        raw_data = pd.read_csv(file_path, usecols=[2, 3, 4, 5], skiprows=skiprows, nrows=row_per_time)\n",
    "        raw_data.columns = ['topic', 'question_title', 'question_content', 'best_answer']\n",
    "        # get labels\n",
    "        labels = raw_data.topic.values\n",
    "\n",
    "        # get corpus\n",
    "        corpus = []\n",
    "        raw_data = raw_data.replace (np.nan, '.')\n",
    "        raw_data['question_title'] = raw_data['question_title'].astype('string')\n",
    "        raw_data['question_content'] = raw_data['question_content'].astype('string')\n",
    "        raw_data['best_answer'] = raw_data['best_answer'].astype('string')\n",
    "        for i in range(len(raw_data)):\n",
    "            if (i+1) % 500 == 0:\n",
    "                print(i+1, end=' ')\n",
    "            sentence = raw_data.iloc[i,1] + \" \" + raw_data.iloc[i,2] + \" \" + raw_data.iloc[i,3]\n",
    "            sentence = lemmatize_sentence(sentence)\n",
    "            corpus.append(sentence)\n",
    "        # save files\n",
    "        save_obj(corpus, \"{}_corpus_{}\".format(output_prefix, skiprows))\n",
    "        save_obj(labels, \"{}_labels_{}\".format(output_prefix, skiprows))\n",
    "        # increament skiprows\n",
    "        skiprows += row_per_time\n",
    "        print()\n",
    "        \n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a2cd5df-c944-42af-8dfd-f462033e9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = os.path.join(\"raw_data\", \"test.csv\")\n",
    "# output_prefix = os.path.join(\"lemmatized_data\", \"test\")\n",
    "# row_per_time = 5000\n",
    "# preprocessing_lemmatize(file_path, output_prefix, row_per_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f0647b5-7ac2-40f1-8977-e0a6a33c83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = os.path.join(\"raw_data\", \"train.csv\")\n",
    "# output_prefix = os.path.join(\"lemmatized_data\", \"train\")\n",
    "# row_per_time = 5000\n",
    "# preprocessing_lemmatize(file_path, output_prefix, row_per_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e77974-512b-4c87-881a-f67afe355cda",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Document term matrix (DTW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2381581e-c49b-4d31-82ee-73041baf2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lemmatized_sentences(filepath_prefix, start_file, end_file, row_per_time):\n",
    "    \"\"\"Load lemmatized sentences\"\"\"\n",
    "    labels = np.array([])\n",
    "    sentences = np.array([])\n",
    "    curr_file = 0\n",
    "    while(curr_file <= end_file):\n",
    "        # load labels\n",
    "        label_filepath = filepath_prefix + \"_labels_\" + str(start_file)\n",
    "        curr_labels = load_obj(label_filepath)\n",
    "        labels = np.concatenate([labels, curr_labels])\n",
    "        # load lemmatized sentences\n",
    "        corpus_filepath = filepath_prefix + \"_corpus_\" + str(start_file)\n",
    "        curr_sentences = np.array(load_obj(corpus_filepath))\n",
    "        sentences = np.concatenate([sentences, curr_sentences])\n",
    "        # increment file number\n",
    "        curr_file += row_per_time\n",
    "    return [sentences, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df0ced7-bd23-418d-bf7d-606216c2145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [[i, sentences[i], sentences[i], labels[i]] for i in range(len(labels))]\n",
    "# data = pd.DataFrame(data, columns=['ID', 'text', 'selected_text', 'labels'])\n",
    "# data.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a20831-e9f2-4a28-8d0f-3187f2580c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [str(int(labels[i])) + '\\t' + sentences[i].replace(' ', ',') + '\\n' for i in range(len(labels))]\n",
    "# with open('test.tab', 'w') as fp:\n",
    "#     for i in range(len(data)):\n",
    "#         fp.write(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cbd54a1-59d4-4dc7-b9e9-d34f995611d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_prefix = os.path.join(\"lemmatized_data\", \"test\")\n",
    "start_file = 0\n",
    "end_file = 0 #55000\n",
    "row_per_time = 5000\n",
    "[sentences, labels] = load_lemmatized_sentences(filepath_prefix, start_file, end_file, row_per_time)\n",
    "sentences = np.array(sentences)[:, np.newaxis]\n",
    "labels = labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93cea802-273b-48c0-ac97-7c0664558d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = sentences\n",
    "y = labels\n",
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\n",
    "X_train_str = X_train_str.reshape(-1)\n",
    "X_test_str = X_test_str.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52e2d2a4-3990-42f1-8584-72d5d7fb4670",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_range, min_df, max_df = (1, 2), 0.005, 0.25\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "vectorizer = CountVectorizer(input='content', decode_error='ignore',\n",
    "                             strip_accents='ascii', lowercase=True,\n",
    "                             stop_words=stopwords, token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_features=1000,\n",
    "                             max_df=max_df, min_df=min_df, ngram_range=ngram_range)\n",
    "X_train_sparse = vectorizer.fit_transform(X_train_str)\n",
    "X_test_sparse = vectorizer.transform(X_test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a96a1072-89e4-4aa1-8947-2d72b4116500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.64375\n",
      "test accuracy: 0.411\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(C=1.0, kernel=\"rbf\")\n",
    "clf = svc.fit(X_train_sparse.toarray(), y_train)\n",
    "print(\"train accuracy:\", clf.score(X_train_sparse.toarray(), y_train))\n",
    "print(\"test accuracy:\", clf.score(X_test_sparse.toarray(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8b8e4f9-220b-4a20-8686-a8e1919bccce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df7a23f2-2860-4369-a515-d0762701c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.estimators.classification import SklearnClassifier\n",
    "# !pip install --upgrade numpy==1.20\n",
    "\n",
    "min_pixel_value = np.amin(X_train_sparse)\n",
    "max_pixel_value = np.amax(X_train_sparse)\n",
    "classifier = SklearnClassifier(model=clf, clip_values=(min_pixel_value, max_pixel_value))\n",
    "\n",
    "# Conduct Fast Gradient Attack on X with strengh epsilon\n",
    "def fgAttack(X, epsilon):\n",
    "    attack = FastGradientMethod(estimator=classifier, eps=epsilon)\n",
    "    X_adv = attack.generate(x=X)\n",
    "    return X_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bca3cabb-430f-49d6-924a-018eaa804751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_sparse.toarray()[:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25ecd85d-9e0a-4251-a7d3-63e41211d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adversarial test examples\n",
    "X_test_adv = fgAttack(X_test_sparse.toarray()[:2, :], 3)\n",
    "\n",
    "# Generate adversarial train examples\n",
    "# X_train_adv = fgAttack(X_train_sparse.toarray(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6f9762f-4b57-4f3d-b8c9-a2cc8989acd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 3., 0., ..., 0., 0., 0.],\n",
       "       [0., 3., 0., ..., 0., 0., 3.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fc715b4-90cc-454a-b645-cf9e26abe8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "687b2655-5ffb-4e48-86ad-2ce00b2d1393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8314"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a9a06-782b-43bf-9e5a-047d7f2258c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15909b3-69a0-4b86-b836-ee79c1120190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
