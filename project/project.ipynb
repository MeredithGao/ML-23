{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7421a2-7c22-47ab-9a7f-67da05dd7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76254c78-a8a9-434e-b602-bd618c547f84",
   "metadata": {},
   "source": [
    "# Download raw data from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982e9c0-0363-4603-a718-da9004ffcab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"yahoo_answers_topics\")\n",
    "# dataset['train'].to_csv(\"raw_data/train.csv\")\n",
    "# dataset['test'].to_csv(\"raw_data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7e0ad-1b1a-4847-b0dd-779be3195870",
   "metadata": {},
   "source": [
    "# Lemmatize sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee275ced-8216-40e7-9a40-84178af61bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos_tagger(nltk_tag):\n",
    "    \"\"\"Add tag about the grammatical category of each word\"\"\"\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    \"\"\"Lemmatize sentence. The returned sentence contains letters only. Other characters are removed.\"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # include letters only r'[a-zA-Z]+'\n",
    "    nltk_tagged = nltk.pos_tag(tokenizer.tokenize(sentence))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_pos_tagger(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    \n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    \"\"\"save as .pickle\"\"\"\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    \"\"\"load .pickle\"\"\"\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74392ff0-5a05-4c9e-9653-221a351b9e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If the light have not be turn off the house would have catch on fire'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example sentence\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatize_sentence(\"If the light had not been turned off, the house would have catched on fire.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88f25bb8-4723-4f7a-ad21-ab670e39c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lemmatize(file_path, output_prefix, row_per_time):\n",
    "    \"\"\"Lemmatize and save sentences in batches.\"\"\"\n",
    "    skiprows = 0 # start from 0 (inclusive)\n",
    "    # total number of rows\n",
    "    num_lines = int(subprocess.check_output(\"wc -l \"+file_path, shell=True).split()[0]) - 1\n",
    "    print(\"Processing {} ({} lines): \".format(file_path, num_lines))\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    while (skiprows < num_lines):\n",
    "        print(\"{} - {}:\".format(skiprows, skiprows+row_per_time), end=\" \")\n",
    "        # read in data\n",
    "        raw_data = pd.read_csv(file_path, usecols=[2, 3, 4, 5], skiprows=skiprows, nrows=row_per_time)\n",
    "        raw_data.columns = ['topic', 'question_title', 'question_content', 'best_answer']\n",
    "        # get labels\n",
    "        labels = raw_data.topic.values\n",
    "\n",
    "        # get corpus\n",
    "        corpus = []\n",
    "        raw_data = raw_data.replace (np.nan, '.')\n",
    "        raw_data['question_title'] = raw_data['question_title'].astype('string')\n",
    "        raw_data['question_content'] = raw_data['question_content'].astype('string')\n",
    "        raw_data['best_answer'] = raw_data['best_answer'].astype('string')\n",
    "        for i in range(len(raw_data)):\n",
    "            if (i+1) % 500 == 0:\n",
    "                print(i+1, end=' ')\n",
    "            sentence = raw_data.iloc[i,1] + \" \" + raw_data.iloc[i,2] + \" \" + raw_data.iloc[i,3]\n",
    "            sentence = lemmatize_sentence(sentence)\n",
    "            corpus.append(sentence)\n",
    "        # save files\n",
    "        save_obj(corpus, \"{}_corpus_{}\".format(output_prefix, skiprows))\n",
    "        save_obj(labels, \"{}_labels_{}\".format(output_prefix, skiprows))\n",
    "        # increament skiprows\n",
    "        skiprows += row_per_time\n",
    "        print()\n",
    "        \n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a2cd5df-c944-42af-8dfd-f462033e9105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\\test.csv (60000 lines): \n",
      "0 - 5000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "5000 - 10000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "10000 - 15000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "15000 - 20000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "20000 - 25000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "25000 - 30000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "30000 - 35000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "35000 - 40000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "40000 - 45000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "45000 - 50000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "50000 - 55000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "55000 - 60000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(\"raw_data\", \"test.csv\")\n",
    "output_prefix = os.path.join(\"lemmatized_data\", \"test\")\n",
    "row_per_time = 5000\n",
    "preprocessing_lemmatize(file_path, output_prefix, row_per_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0647b5-7ac2-40f1-8977-e0a6a33c83db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\\train.csv (1400000 lines): \n",
      "0 - 5000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "5000 - 10000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "10000 - 15000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "15000 - 20000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "20000 - 25000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "25000 - 30000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "30000 - 35000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "35000 - 40000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "40000 - 45000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "45000 - 50000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "50000 - 55000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "55000 - 60000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "60000 - 65000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "65000 - 70000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "70000 - 75000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "75000 - 80000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "80000 - 85000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "85000 - 90000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "90000 - 95000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "95000 - 100000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "100000 - 105000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "105000 - 110000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "110000 - 115000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "115000 - 120000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "120000 - 125000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "125000 - 130000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "130000 - 135000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "135000 - 140000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "140000 - 145000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "145000 - 150000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "150000 - 155000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "155000 - 160000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "160000 - 165000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "165000 - 170000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "170000 - 175000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "175000 - 180000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "180000 - 185000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "185000 - 190000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "190000 - 195000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "195000 - 200000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "200000 - 205000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "205000 - 210000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "210000 - 215000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "215000 - 220000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "220000 - 225000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "225000 - 230000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "230000 - 235000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "235000 - 240000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "240000 - 245000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "245000 - 250000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "250000 - 255000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "255000 - 260000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "260000 - 265000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "265000 - 270000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "270000 - 275000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "275000 - 280000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "280000 - 285000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "285000 - 290000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "290000 - 295000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "295000 - 300000: 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 \n",
      "300000 - 305000: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\wenxu/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16388/1299408934.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moutput_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrow_per_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpreprocessing_lemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_per_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16388/2497566593.py\u001b[0m in \u001b[0;36mpreprocessing_lemmatize\u001b[1;34m(file_path, output_prefix, row_per_time)\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatize_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# save files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16388/108642252.py\u001b[0m in \u001b[0;36mlemmatize_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;34m\"\"\"Lemmatize sentence. The returned sentence contains letters only. Other characters are removed.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# include letters only r'[a-zA-Z]+'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mnltk_tagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mwordnet_tagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnltk_pos_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnltk_tagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mlemmatized_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m             )\n\u001b[0;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;31m# Is the path item a zipfile?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpath_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(\"raw_data\", \"train.csv\")\n",
    "output_prefix = os.path.join(\"lemmatized_data\", \"train\")\n",
    "row_per_time = 5000\n",
    "preprocessing_lemmatize(file_path, output_prefix, row_per_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "654e5b4e-8b85-4e83-8a2a-1f9a83c9e6e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_content</th>\n",
       "      <th>best_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>What makes friendship click?</td>\n",
       "      <td>How does the spark keep going?</td>\n",
       "      <td>good communication is what does it.  Can you m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Why does Zebras have stripes?</td>\n",
       "      <td>What is the purpose or those stripes? Who do t...</td>\n",
       "      <td>this provides camouflage - predator vision is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What did the itsy bitsy sipder climb up?</td>\n",
       "      <td>.</td>\n",
       "      <td>waterspout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the difference between a Bachelors and...</td>\n",
       "      <td>.</td>\n",
       "      <td>One difference between a Bachelors and a Maste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Why do women get PMS?</td>\n",
       "      <td>.</td>\n",
       "      <td>Premenstrual syndrome (PMS) is a group of symp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3</td>\n",
       "      <td>Hay um can some1 pleaseeee tell me a bone that...</td>\n",
       "      <td>plzzz i really need this fro a project!!!!!!!!</td>\n",
       "      <td>The Xiphoid, attached to the sternum is bone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>6</td>\n",
       "      <td>Do you belive that the world can go through an...</td>\n",
       "      <td>.</td>\n",
       "      <td>I think yes! due to the technology we have, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>7</td>\n",
       "      <td>Looking for Charlotte's Web DVD in Spanish? do...</td>\n",
       "      <td>.</td>\n",
       "      <td>No, both the full-screen (ASIN: B00005N89B) an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>when i get some e mails, instead of a picture ...</td>\n",
       "      <td>.</td>\n",
       "      <td>it's b/c it's in HTML. at the bottom of the e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>6</td>\n",
       "      <td>what does a japanese keyboard look like?</td>\n",
       "      <td>.</td>\n",
       "      <td>Like THAT (1st link) ^_^. It looks like our ke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic                                     question_title  \\\n",
       "0       8                       What makes friendship click?   \n",
       "1       1                      Why does Zebras have stripes?   \n",
       "2       3           What did the itsy bitsy sipder climb up?   \n",
       "3       3  What is the difference between a Bachelors and...   \n",
       "4       2                              Why do women get PMS?   \n",
       "..    ...                                                ...   \n",
       "95      3  Hay um can some1 pleaseeee tell me a bone that...   \n",
       "96      6  Do you belive that the world can go through an...   \n",
       "97      7  Looking for Charlotte's Web DVD in Spanish? do...   \n",
       "98      4  when i get some e mails, instead of a picture ...   \n",
       "99      6           what does a japanese keyboard look like?   \n",
       "\n",
       "                                     question_content  \\\n",
       "0                      How does the spark keep going?   \n",
       "1   What is the purpose or those stripes? Who do t...   \n",
       "2                                                   .   \n",
       "3                                                   .   \n",
       "4                                                   .   \n",
       "..                                                ...   \n",
       "95     plzzz i really need this fro a project!!!!!!!!   \n",
       "96                                                  .   \n",
       "97                                                  .   \n",
       "98                                                  .   \n",
       "99                                                  .   \n",
       "\n",
       "                                          best_answer  \n",
       "0   good communication is what does it.  Can you m...  \n",
       "1   this provides camouflage - predator vision is ...  \n",
       "2                                          waterspout  \n",
       "3   One difference between a Bachelors and a Maste...  \n",
       "4   Premenstrual syndrome (PMS) is a group of symp...  \n",
       "..                                                ...  \n",
       "95  The Xiphoid, attached to the sternum is bone w...  \n",
       "96  I think yes! due to the technology we have, al...  \n",
       "97  No, both the full-screen (ASIN: B00005N89B) an...  \n",
       "98  it's b/c it's in HTML. at the bottom of the e-...  \n",
       "99  Like THAT (1st link) ^_^. It looks like our ke...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd54a1-59d4-4dc7-b9e9-d34f995611d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
